# ğŸ’¾ SystÃ¨me de Cache pour le Moteur de Recherche

## ğŸ“‹ AperÃ§u

Ce systÃ¨me de cache optimise les performances du moteur de recherche en mÃ©morisant les rÃ©sultats des requÃªtes. GrÃ¢ce Ã  un stockage ultra-compact (seulement les IDs et scores), il peut cacher **des centaines de milliers de requÃªtes dans 1 Go de RAM**.

## ğŸ¯ Architecture

### Double couche de cache
- **RAM** : Cache LRU (Least Recently Used) ultra-rapide, limitÃ© Ã  10 000 entrÃ©es par dÃ©faut
- **Disque** : Cache persistant illimitÃ© dans chaque index (`cache.json`)

### Taille d'une entrÃ©e cachÃ©e
Chaque requÃªte cachÃ©e stocke :
- 15 rÃ©sultats Ã— (2 IDs de 32 chars + 1 score float)
- **â‰ˆ 500 bytes par requÃªte**
- **1 Go = ~2 millions de requÃªtes** ğŸš€

## ğŸ“ Structure des fichiers

```
all_indexes/
â””â”€â”€ ma_bibliotheque/
    â”œâ”€â”€ index/              # Index FAISS
    â”œâ”€â”€ source_files_archive/
    â”œâ”€â”€ md_files/
    â””â”€â”€ cache.json          # âœ¨ NOUVEAU : Cache disque
```

## ğŸ”§ Installation

### 1. Ajouter le module cache

Copiez `cache.py` dans `src/core/cache.py`.

### 2. Modifier `search.py`

Remplacez votre fichier `src/routes/search.py` par la version modifiÃ©e fournie, ou appliquez manuellement les modifications suivantes :

**Import** (ligne ~17) :
```python
from src.core.cache import search_cache
```

**Dans la fonction `search_in_index`**, AVANT le pipeline de recherche :
```python
# VÃ©rifier le cache
cached_results = search_cache.get(
    query=request.query,
    index_id=index_id,
    index_path=index_path,
    user_groups=request.user_groups
)

if cached_results is not None:
    # Reconstruire les rÃ©sultats depuis les IDs cachÃ©s
    # ... (voir le code complet)
    return results
```

**Ã€ la fin du pipeline de recherche**, sauvegarder dans le cache :
```python
# PrÃ©parer les donnÃ©es pour le cache
cache_data = [
    (child_node.id_, parent_node.id_, pair['rerank_score'])
    for pair in final_pairs
]

# Sauvegarder dans le cache
search_cache.set(
    query=request.query,
    index_id=index_id,
    index_path=index_path,
    user_groups=request.user_groups,
    results=cache_data
)
```

### 3. Modifier `indexing.py`

Ouvrez `src/core/indexing.py` et appliquez les modifications du fichier `indexing_modifications.txt` :

**Import** (ligne ~30) :
```python
from src.core.cache import search_cache
```

**Dans `index_creation_task`**, aprÃ¨s la crÃ©ation du status_file (ligne ~370) :
```python
# Nettoyer le cache pour cet index lors de la rÃ©indexation
logger.info(f"ğŸ—‘ï¸  Clearing cache for index: {index_id}")
search_cache.clear_index_cache(index_path)
```

## ğŸš€ Utilisation

### Pipeline automatique

Le cache fonctionne automatiquement :

1. **RequÃªte entrante** â†’ VÃ©rification cache RAM
2. **Cache RAM miss** â†’ VÃ©rification cache disque
3. **Cache disque hit** â†’ Chargement en RAM + reconstruction des rÃ©sultats
4. **Cache complet miss** â†’ Pipeline de recherche complet + sauvegarde dans le cache

### Normalisation des requÃªtes

Les requÃªtes sont automatiquement normalisÃ©es pour maximiser les cache hits :
```python
"Qu'est-ce que   l'IA ?"  â†’ "qu'est-ce que l'ia ?"
"  Machine Learning  "    â†’ "machine learning"
```

## ğŸ“Š Monitoring

### Statistiques du cache

**Endpoint** : `GET /search/{index_id}/cache/stats`

**RÃ©ponse** :
```json
{
  "cache_stats": {
    "ram_hits": 1250,
    "disk_hits": 380,
    "misses": 420,
    "writes": 420
  },
  "total_requests": 2050,
  "hit_rate_percentage": 79.51,
  "ram_cache_size": 1630
}
```

### InterprÃ©tation
- **ram_hits** : RequÃªtes servies depuis la RAM (< 1ms)
- **disk_hits** : RequÃªtes servies depuis le disque (5-10ms)
- **misses** : Nouvelles requÃªtes nÃ©cessitant le pipeline complet (500-2000ms)
- **hit_rate_percentage** : Pourcentage de requÃªtes cachÃ©es

## ğŸ› ï¸ Administration

### Vider le cache d'un index

**Endpoint** : `DELETE /search/{index_id}/cache`

```bash
curl -X DELETE "https://api.example.com/search/ma_bibliotheque/cache" \
  -H "X-API-Key: votre_api_key"
```

**UtilitÃ©** :
- AprÃ¨s une rÃ©indexation (fait automatiquement)
- Pour libÃ©rer de l'espace disque
- Pour forcer le recalcul des rÃ©sultats

### Vider le cache RAM global

```python
from src.core.cache import search_cache

# Vider complÃ¨tement la RAM
search_cache.clear_all_ram()

# RÃ©initialiser les stats
search_cache.stats = {
    "ram_hits": 0,
    "disk_hits": 0,
    "misses": 0,
    "writes": 0
}
```

## âš™ï¸ Configuration

### Taille du cache RAM

Par dÃ©faut : 10 000 entrÃ©es (â‰ˆ 5 Mo)

Modifier dans `src/core/cache.py` :
```python
# Instance globale
search_cache = SearchCache(max_ram_entries=20000)  # 20k entrÃ©es
```

### ClÃ© de cache

Format : `SHA256(query_normalisÃ©e|index_id|user_groups_triÃ©s)[:16]`

Exemples :
```
query="machine learning", index_id="ai_docs", groups=["public"]
â†’ clÃ©: "a3f2e9b7d4c1"

query="deep learning", index_id="ai_docs", groups=["admin","user"]
â†’ clÃ©: "8c4d1a5e7f2b"
```

## ğŸ” DÃ©tails techniques

### Structure du cache disque (cache.json)

```json
{
  "a3f2e9b7d4c1": [
    ["child_id_1", "parent_id_1", 0.95],
    ["child_id_2", "parent_id_2", 0.89],
    ...
  ],
  "8c4d1a5e7f2b": [
    ...
  ]
}
```

### Reconstruction depuis le cache

Pour chaque tuple `(child_id, parent_id, score)` :
1. Charger `child_node` depuis le docstore
2. Charger `parent_node` depuis le docstore
3. Extraire `precise_content` (du child)
4. Extraire `context_content` (du parent)
5. Construire `SearchResultNode` complet

**Temps** : ~5-10ms pour 15 rÃ©sultats (vs 500-2000ms pour le pipeline complet)

### Thread safety

Le cache RAM utilise un `Lock` pour garantir la thread safety :
```python
with self.lock:
    self.ram_cache[cache_key] = result
```

## ğŸ“ˆ Gains de performance attendus

### ScÃ©nario typique

**Avant le cache** :
- Recherche : 500-2000ms
- Throughput : 1-2 requÃªtes/seconde

**AprÃ¨s le cache (80% hit rate)** :
- Cache RAM hit : <1ms
- Cache disque hit : 5-10ms
- Throughput : 50-200 requÃªtes/seconde

### ROI

- **Stockage** : 1 Go RAM = 2 millions de requÃªtes
- **Latence** : RÃ©duction de 99% pour les requÃªtes cachÃ©es
- **Serveur** : Ã‰conomie massive de CPU/GPU (pas de reranking)

## ğŸ› DÃ©pannage

### Cache hit rate faible (<30%)

**Causes possibles** :
1. RequÃªtes trop variÃ©es (typos, formulations diffÃ©rentes)
2. Groupes utilisateurs trÃ¨s fragmentÃ©s
3. Cache RAM trop petit

**Solutions** :
- Augmenter `max_ram_entries`
- ImplÃ©menter une normalisation plus agressive
- Analyser les patterns de requÃªtes

### Fichier cache.json volumineux

**Taille normale** : 100k-1M de requÃªtes = 50-500 Mo

**Si trop gros** :
```python
# Option 1 : Supprimer et recrÃ©er
os.remove(f"{index_path}/cache.json")

# Option 2 : Filtrer les anciennes entrÃ©es (Ã  implÃ©menter)
```

### RÃ©sultats incohÃ©rents aprÃ¨s rÃ©indexation

**Cause** : Cache non vidÃ© aprÃ¨s rÃ©indexation

**Solution** : Le cache est automatiquement vidÃ© si vous avez appliquÃ© les modifications dans `indexing.py`

## ğŸ” SÃ©curitÃ©

### Isolation par groupes

Les rÃ©sultats sont cachÃ©s **par combinaison de groupes utilisateur** :
- User A (groups: ["admin", "dev"]) ne verra pas le cache de User B (groups: ["user"])
- Garantit la confidentialitÃ© mÃªme avec des requÃªtes identiques

### Format de la clÃ©

```python
cache_key = hash(query + index_id + sorted(user_groups))
```

Aucune collision possible entre utilisateurs de groupes diffÃ©rents.

## ğŸ“ Logs

### Lors d'un cache hit (RAM)
```
ğŸ’¾ Cache RAM HIT for query: 'machine learning...' (key: a3f2e9b7)
âœ¨ Cache HIT! Rebuilding 15 results from cached IDs
âœ… Cache reconstruction complete: 15 results
```

### Lors d'un cache hit (Disque)
```
ğŸ’¿ Cache DISK HIT for query: 'deep learning...' (key: 8c4d1a5e)
âœ¨ Cache HIT! Rebuilding 15 results from cached IDs
âœ… Cache reconstruction complete: 15 results
```

### Lors d'un cache miss
```
ğŸ” Cache MISS for query: 'reinforcement learning...' (key: f3a8b2c1)
ğŸ” Cache MISS! Running full search pipeline
ğŸ“ STEP 1: Retrieving sub-chunks...
...
ğŸ’¾ Cached query: 'reinforcement learning...' (key: f3a8b2c1, 15 results)
```

## ğŸ“ Exemple d'intÃ©gration complÃ¨te

Voir les fichiers fournis :
- `cache.py` : Module de cache complet
- `search.py` : Recherche avec intÃ©gration du cache
- `indexing_modifications.txt` : Modifications pour l'indexation

## ğŸš¦ Checklist de dÃ©ploiement

- [ ] Copier `cache.py` dans `src/core/`
- [ ] Remplacer `search.py` ou appliquer les modifications
- [ ] Modifier `indexing.py` (import + clear_index_cache)
- [ ] Tester avec une requÃªte simple
- [ ] VÃ©rifier les stats : `GET /search/{index_id}/cache/stats`
- [ ] Monitorer les logs pour voir les cache hits
- [ ] RÃ©indexer une bibliothÃ¨que et vÃ©rifier que le cache est vidÃ©
- [ ] Tester avec diffÃ©rents groupes utilisateurs

## ğŸ“š Ressources

- **Documentation LlamaIndex** : https://docs.llamaindex.ai/
- **FAISS** : https://github.com/facebookresearch/faiss
- **OrderedDict LRU** : https://docs.python.org/3/library/collections.html#collections.OrderedDict

## ğŸ¤ Support

Pour toute question ou problÃ¨me :
1. VÃ©rifier les logs du serveur
2. Consulter les stats du cache
3. Tester avec `curl` direct
4. Vider le cache et retenter

---

**Version** : 1.0.0  
**Date** : Octobre 2025  
**Auteur** : SystÃ¨me de cache pour moteur de recherche sÃ©mantique